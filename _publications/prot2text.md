---
title: "Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers"
collection: publications
permalink: /publication/prot2text
excerpt: "We propose Prot2Text, which predicts a protein function's in a free text style, moving beyond the conventional binary or categorical classifications. By combining Graph Neural Networks(GNNs) and Large Language Models(LLMs), in an encoder-decoder framework, our model effectively integrates diverse data types including proteins' sequences, structures, and textual annotations. [Read More](https://michailchatzianastasis.github.io/publication/prot2text)"
date: '2023-10-27'
venue: ' <a href="https://sites.google.com/ethz.ch/dgm4h-neurips2023/home?authuser=0">Spotlight at DGM4H Neurips 2023</a> and <a href="https://ai4sciencecommunity.github.io/neurips23.html">AIforScience Neurips 2023</a>'
paperurl: 'https://arxiv.org/abs/2307.14367'
citation: 'Hadi Abdine, <strong>Michail Chatzianastasis</strong>, Costas Bouyioukos, Michalis Vazirgiannis'
abstract: "Graph neural networks have become the standard approach for dealing with learning problems on graphs. Among the different variants of graph neural networks, graph attention networks (GATs) have been applied with great success to different tasks. In the GAT model, each node assigns an importance score to its neighbors using an attention mechanism. However, similar to other graph neural networks, GATs aggregate messages from nodes that belong to different classes, and therefore produce node representations that are not well separated with respect to the different classes, which might hurt their performance. In this work, to alleviate this problem, we propose a new technique that can be incorporated into any graph attention model to encourage higher attention scores between nodes that share the same class label. We evaluate the proposed method on several node classification datasets demonstrating increased performance over standard baseline models.
"
---
